{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87ea6e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import sklearn\n",
    "from sklearn import datasets, metrics\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator as IDG\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator as IDG\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1eb19ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x216 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "digits = datasets.load_digits()\n",
    "\n",
    "_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))\n",
    "for ax, image, label in zip(axes, digits.images, digits.target):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "    ax.set_title(\"Training: %i\" % label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc226fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "        [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n",
       "        [ 0.,  0., 10., ..., 12.,  1.,  0.]]),\n",
       " 'target': array([0, 1, 2, ..., 8, 9, 8]),\n",
       " 'frame': None,\n",
       " 'feature_names': ['pixel_0_0',\n",
       "  'pixel_0_1',\n",
       "  'pixel_0_2',\n",
       "  'pixel_0_3',\n",
       "  'pixel_0_4',\n",
       "  'pixel_0_5',\n",
       "  'pixel_0_6',\n",
       "  'pixel_0_7',\n",
       "  'pixel_1_0',\n",
       "  'pixel_1_1',\n",
       "  'pixel_1_2',\n",
       "  'pixel_1_3',\n",
       "  'pixel_1_4',\n",
       "  'pixel_1_5',\n",
       "  'pixel_1_6',\n",
       "  'pixel_1_7',\n",
       "  'pixel_2_0',\n",
       "  'pixel_2_1',\n",
       "  'pixel_2_2',\n",
       "  'pixel_2_3',\n",
       "  'pixel_2_4',\n",
       "  'pixel_2_5',\n",
       "  'pixel_2_6',\n",
       "  'pixel_2_7',\n",
       "  'pixel_3_0',\n",
       "  'pixel_3_1',\n",
       "  'pixel_3_2',\n",
       "  'pixel_3_3',\n",
       "  'pixel_3_4',\n",
       "  'pixel_3_5',\n",
       "  'pixel_3_6',\n",
       "  'pixel_3_7',\n",
       "  'pixel_4_0',\n",
       "  'pixel_4_1',\n",
       "  'pixel_4_2',\n",
       "  'pixel_4_3',\n",
       "  'pixel_4_4',\n",
       "  'pixel_4_5',\n",
       "  'pixel_4_6',\n",
       "  'pixel_4_7',\n",
       "  'pixel_5_0',\n",
       "  'pixel_5_1',\n",
       "  'pixel_5_2',\n",
       "  'pixel_5_3',\n",
       "  'pixel_5_4',\n",
       "  'pixel_5_5',\n",
       "  'pixel_5_6',\n",
       "  'pixel_5_7',\n",
       "  'pixel_6_0',\n",
       "  'pixel_6_1',\n",
       "  'pixel_6_2',\n",
       "  'pixel_6_3',\n",
       "  'pixel_6_4',\n",
       "  'pixel_6_5',\n",
       "  'pixel_6_6',\n",
       "  'pixel_6_7',\n",
       "  'pixel_7_0',\n",
       "  'pixel_7_1',\n",
       "  'pixel_7_2',\n",
       "  'pixel_7_3',\n",
       "  'pixel_7_4',\n",
       "  'pixel_7_5',\n",
       "  'pixel_7_6',\n",
       "  'pixel_7_7'],\n",
       " 'target_names': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " 'images': array([[[ 0.,  0.,  5., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 13., ..., 15.,  5.,  0.],\n",
       "         [ 0.,  3., 15., ..., 11.,  8.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  4., 11., ..., 12.,  7.,  0.],\n",
       "         [ 0.,  2., 14., ..., 12.,  0.,  0.],\n",
       "         [ 0.,  0.,  6., ...,  0.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  0., ...,  5.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  9.,  0.,  0.],\n",
       "         [ 0.,  0.,  3., ...,  6.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "         [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ..., 10.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  0., ..., 12.,  0.,  0.],\n",
       "         [ 0.,  0.,  3., ..., 14.,  0.,  0.],\n",
       "         [ 0.,  0.,  8., ..., 16.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  9., 16., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  3., 13., ..., 11.,  5.,  0.],\n",
       "         [ 0.,  0.,  0., ..., 16.,  9.,  0.]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.,  0.,  1., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 13., ...,  2.,  1.,  0.],\n",
       "         [ 0.,  0., 16., ..., 16.,  5.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0., 16., ..., 15.,  0.,  0.],\n",
       "         [ 0.,  0., 15., ..., 16.,  0.,  0.],\n",
       "         [ 0.,  0.,  2., ...,  6.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  2., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0., 14., ..., 15.,  1.,  0.],\n",
       "         [ 0.,  4., 16., ..., 16.,  7.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ..., 16.,  2.,  0.],\n",
       "         [ 0.,  0.,  4., ..., 16.,  2.,  0.],\n",
       "         [ 0.,  0.,  5., ..., 12.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0., 10., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  2., 16., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 15., ..., 15.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  4., 16., ..., 16.,  6.,  0.],\n",
       "         [ 0.,  8., 16., ..., 16.,  8.,  0.],\n",
       "         [ 0.,  1.,  8., ..., 12.,  1.,  0.]]]),\n",
       " 'DESCR': \".. _digits_dataset:\\n\\nOptical recognition of handwritten digits dataset\\n--------------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 1797\\n    :Number of Attributes: 64\\n    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\\n    :Missing Attribute Values: None\\n    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\\n    :Date: July; 1998\\n\\nThis is a copy of the test set of the UCI ML hand-written digits datasets\\nhttps://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\\n\\nThe data set contains images of hand-written digits: 10 classes where\\neach class refers to a digit.\\n\\nPreprocessing programs made available by NIST were used to extract\\nnormalized bitmaps of handwritten digits from a preprinted form. From a\\ntotal of 43 people, 30 contributed to the training set and different 13\\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\\n4x4 and the number of on pixels are counted in each block. This generates\\nan input matrix of 8x8 where each element is an integer in the range\\n0..16. This reduces dimensionality and gives invariance to small\\ndistortions.\\n\\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\\n1994.\\n\\n.. topic:: References\\n\\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\\n    Graduate Studies in Science and Engineering, Bogazici University.\\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\\n    Linear dimensionalityreduction using relevance weighted LDA. School of\\n    Electrical and Electronic Engineering Nanyang Technological University.\\n    2005.\\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\\n    Algorithm. NIPS. 2000.\\n\"}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6240c47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classNo = digits['target_names']\n",
    "noOfClasses = len(digits['target_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39c0578f",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.array(digits.images)\n",
    "classNo = np.array(digits.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48419407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1797, 8, 8), (1797,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape, classNo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d74098d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratio = 0.1\n",
    "val_ratio = 0.1\n",
    "imageDimensions = (8,8,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c539f1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = tts(images, classNo, test_size = test_ratio)\n",
    "x_train, x_val, y_train, y_val = tts(x_train, y_train, test_size = val_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3765c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes :  [0 1 2 ... 8 9 8]\n",
      "shape of raw image dataset :  (1797, 8, 8)\n",
      "shape of train dataset :  (1455, 8, 8)\n",
      "shape of test dataset :  (180, 8, 8)\n",
      "shape of validation dataset :  (162, 8, 8)\n"
     ]
    }
   ],
   "source": [
    "print(\"Classes : \", classNo)\n",
    "print(\"shape of raw image dataset : \",images.shape)\n",
    "print(\"shape of train dataset : \",x_train.shape)\n",
    "print(\"shape of test dataset : \",x_test.shape)\n",
    "print(\"shape of validation dataset : \",x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee56ea05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of images having class = 0 is 145\n",
      "no. of images having class = 1 is 149\n",
      "no. of images having class = 2 is 151\n",
      "no. of images having class = 3 is 148\n",
      "no. of images having class = 4 is 150\n",
      "no. of images having class = 5 is 141\n",
      "no. of images having class = 6 is 135\n",
      "no. of images having class = 7 is 148\n",
      "no. of images having class = 8 is 135\n",
      "no. of images having class = 9 is 153\n",
      "\n",
      " [145, 149, 151, 148, 150, 141, 135, 148, 135, 153]\n"
     ]
    }
   ],
   "source": [
    "noOfSamples = []\n",
    "for x in range(0, noOfClasses):\n",
    "    print(f\"no. of images having class = {x} is\",len(np.where(y_train==x)[0]))\n",
    "    noOfSamples.append(len(np.where(y_train==x)[0]))\n",
    "print('\\n',noOfSamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7174d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(range(0,noOfClasses), noOfSamples)\n",
    "plt.title(\"No of images for each class\")\n",
    "plt.xlabel(\"class ID\", color='w')\n",
    "plt.ylabel(\"No. of images\", color='w')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "42a87c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "Scaler_standard = StandardScaler()\n",
    "Scaler_minmax = MinMaxScaler()\n",
    "\n",
    "def preProcessing(img):\n",
    "#     img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "#     img = cv2.equalizeHist(img)\n",
    "    scaled_img = img/12\n",
    "\n",
    "#     scaled_img = Scaler_standard.fit_transform(img)\n",
    "#     scaled_img = Scaler_minmax.fit_transform(img)\n",
    "    \n",
    "#     scaled_img_1 = Scaler_standard.fit_transform(img)\n",
    "#     scaled_img = Scaler_minmax.fit_transform(scaled_img_1)\n",
    "    return scaled_img\n",
    "\n",
    "def preProcessing_S(img):\n",
    "    scaled_img = Scaler_standard.fit_transform(img)\n",
    "    return scaled_img\n",
    "\n",
    "def preProcessing_N(img):\n",
    "    scaled_img = Scaler_minmax.fit_transform(img)\n",
    "    return scaled_img\n",
    "\n",
    "def preProcessing_N_S(img):    \n",
    "    scaled_img_1 = Scaler_minmax.fit_transform(img)\n",
    "    scaled_img = Scaler_standard.fit_transform(scaled_img_1)\n",
    "    return scaled_img\n",
    "\n",
    "def preProcessing_S_N(img):    \n",
    "    scaled_img_1 = Scaler_standard.fit_transform(img)\n",
    "    scaled_img = Scaler_minmax.fit_transform(scaled_img_1)\n",
    "    return scaled_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "02dff440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img0 = preProcessing(x_train[70])\n",
    "img1 = preProcessing_N(x_train[70])\n",
    "img2 = preProcessing_S(x_train[70])\n",
    "img3 = preProcessing_N_S(x_train[70])\n",
    "img4 = preProcessing_S_N(x_train[70])\n",
    "\n",
    "img0 = cv2.resize(img0,(300,300))\n",
    "img1 = cv2.resize(img1,(300,300))\n",
    "img2 = cv2.resize(img2,(300,300))\n",
    "img3 = cv2.resize(img3,(300,300))\n",
    "img4 = cv2.resize(img4,(300,300))\n",
    "\n",
    "cv2.imshow(\"PreProcessed Image\", img0)\n",
    "cv2.imshow(\"PreProcessed Image (N)\", img1)\n",
    "cv2.imshow(\"PreProcessed Image (S)\", img2)\n",
    "cv2.imshow(\"PreProcessed Image (N_S)\", img3)\n",
    "cv2.imshow(\"PreProcessed Image (S_N)\", img4)\n",
    "\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e59468bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(list(map(preProcessing_S_N,x_train)))\n",
    "x_test = np.array(list(map(preProcessing_S_N,x_test)))\n",
    "x_val = np.array(list(map(preProcessing_S_N,x_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9961a50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0],x_train.shape[1],x_train.shape[2],1)\n",
    "x_val = x_val.reshape(x_val.shape[0],x_val.shape[1],x_val.shape[2],1)\n",
    "x_test = x_test.reshape(x_test.shape[0],x_test.shape[1],x_test.shape[2],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4119c186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1455, 8, 8, 1)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "47a0e4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataGen = IDG(\n",
    "            width_shift_range=0.1,\n",
    "            height_shift_range=0.1,\n",
    "            zoom_range=0.2,\n",
    "            shear_range=0.1,\n",
    "            rotation_range=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0920bc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataGen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "991d1023",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train,noOfClasses)\n",
    "y_test = to_categorical(y_test,noOfClasses)\n",
    "y_val = to_categorical(y_val,noOfClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fd5dc401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LENET Model\n",
    "def myModel():\n",
    "    noOfFilters = 60\n",
    "    sizeOfFilter1 = (5,5)\n",
    "    sizeOfFilter2 = (3,3)\n",
    "    sizeOfPool = (2,2)\n",
    "    noOfNode = 500\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add((Conv2D(noOfFilters,sizeOfFilter1, input_shape=(imageDimensions[0],imageDimensions[1],imageDimensions[2]), activation='relu')))\n",
    "    model.add((Conv2D(noOfFilters,sizeOfFilter1, activation='relu')))\n",
    "    model.add(MaxPooling2D(pool_size=sizeOfPool))\n",
    "    model.add((Conv2D(noOfFilters//2,sizeOfFilter2, activation='relu')))\n",
    "    model.add((Conv2D(noOfFilters//2,sizeOfFilter2, activation='relu')))\n",
    "    model.add(MaxPooling2D(pool_size=sizeOfPool))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(noOfNode,activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(noOfClasses,activation='softmax'))\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2d905bac",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"conv2d_1\" (type Conv2D).\n\nNegative dimension size caused by subtracting 5 from 4 for '{{node conv2d_1/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](Placeholder, conv2d_1/Conv2D/ReadVariableOp)' with input shapes: [?,4,4,60], [5,5,60,60].\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(None, 4, 4, 60), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-48f891bc88ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmyModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-69-c1ce05df9230>\u001b[0m in \u001b[0;36mmyModel\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnoOfFilters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msizeOfFilter1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimageDimensions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimageDimensions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimageDimensions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnoOfFilters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msizeOfFilter1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msizeOfPool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnoOfFilters\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msizeOfFilter2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    528\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[0;32m   1937\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1938\u001b[0m     \u001b[1;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1939\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1940\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1941\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"conv2d_1\" (type Conv2D).\n\nNegative dimension size caused by subtracting 5 from 4 for '{{node conv2d_1/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](Placeholder, conv2d_1/Conv2D/ReadVariableOp)' with input shapes: [?,4,4,60], [5,5,60,60].\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(None, 4, 4, 60), dtype=float32)"
     ]
    }
   ],
   "source": [
    "model = myModel()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9143ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSizeVal = 50\n",
    "epochsVal = 10\n",
    "stepsPerEpochVal = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6c4de299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " Computed output size would be negative: -2 [input_size: 0, effective_filter_size: 3, stride: 1]\n\t [[node sequential_3/conv2d_12/Relu (defined at <ipython-input-33-dd730996f8e7>:1) ]] [Op:__inference_train_function_2747]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-dd730996f8e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m history = model.fit(dataGen.flow(x_train,y_train,\n\u001b[0m\u001b[0;32m      2\u001b[0m                                batch_size=batchSizeVal),\n\u001b[0;32m      3\u001b[0m                                \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstepsPerEpochVal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochsVal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    948\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1963\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m:  Computed output size would be negative: -2 [input_size: 0, effective_filter_size: 3, stride: 1]\n\t [[node sequential_3/conv2d_12/Relu (defined at <ipython-input-33-dd730996f8e7>:1) ]] [Op:__inference_train_function_2747]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataGen.flow(x_train,y_train,\n",
    "                               batch_size=batchSizeVal),\n",
    "                               steps_per_epoch=stepsPerEpochVal,\n",
    "                               epochs=epochsVal,\n",
    "                               validation_data=(x_val,y_val),\n",
    "                               shuffle=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "594f3c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.0'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6236c6ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
